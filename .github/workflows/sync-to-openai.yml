name: Sync Repo to OpenAI Storage (per-file, deduped)

on:
  workflow_call:
    inputs:
      vector_store_id:
        required: true
        type: string
    secrets:
      OPENAI_API_KEY:
        required: true

concurrency:
  group: sync-openai-${{ github.repository }}
  cancel-in-progress: true

jobs:
  sync-repo:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-node@v4
        with:
          node-version: 20

      - run: npm i openai@^4

      - name: Upload files one by one (dedupe + skip unchanged)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          VECTOR_STORE_ID: ${{ inputs.vector_store_id }}
        run: |
          node --input-type=module - <<'NODE'
          import fs from "fs";
          import path from "path";
          import crypto from "crypto";
          import { execSync } from "child_process";
          import OpenAI from "openai";

          const apiKey   = process.env.OPENAI_API_KEY;
          const vsId     = process.env.VECTOR_STORE_ID;
          const repoName = (process.env.GITHUB_REPOSITORY || "").split("/").pop();

          if (!apiKey) throw new Error("Missing OPENAI_API_KEY");
          if (!vsId)   throw new Error("Missing VECTOR_STORE_ID");

          const client = new OpenAI({ apiKey });

          const listFiles = () =>
            execSync("git ls-files", { encoding: "utf8" })
              .split("\n")
              .filter(Boolean)
              .filter(f =>
                !f.startsWith(".git/") &&
                !f.startsWith(".github/workflows/") &&
                !f.includes("node_modules/") &&
                !/\.(png|jpg|jpeg|gif|svg|ico|pdf|zip|gz|tar|lock|woff2?|ttf|eot)$/i.test(f)
              );

          const sha256 = (p) => crypto.createHash("sha256").update(fs.readFileSync(p)).digest("hex");

          // paginate over ALL vector-store files, yielding pages
          async function* pages(params = {}) {
            let after;
            do {
              const res = await client.vectorStores.files.list(vsId, { after, limit: 100, ...params });
              yield res;
              after = res?.last_id;
            } while (after);
          }

          // return ALL store files whose filename matches exactly
          async function getAllByFilename(name) {
            const matches = [];
            for await (const page of pages()) {
              for (const f of page.data) if (f.filename === name) matches.push(f);
            }
            return matches;
          }

          // delete ALL files by filename (every page)
          async function deleteAllByFilename(name) {
            const all = await getAllByFilename(name);
            for (const f of all) {
              try {
                await client.vectorStores.files.del(vsId, f.id);
                console.log("Deleted old copy:", f.id, f.filename);
              } catch (e) {
                console.warn("Failed to delete", f.id, e?.response?.data || e?.message || e);
              }
            }
          }

          // find an existing file with same filename AND same sha (to skip upload)
          async function hasSameHash(name, sha) {
            for await (const page of pages()) {
              for (const f of page.data) {
                if (f.filename === name && f.attributes && f.attributes.sha256 === sha) {
                  return true;
                }
              }
            }
            return false;
          }

          const files = listFiles();
          console.log(`Found ${files.length} candidate files`);

          (async () => {
            for (const rel of files) {
              const abs = path.resolve(rel);
              const st = fs.statSync(abs);
              if (!st.isFile() || st.size > 2 * 1024 * 1024) {
                console.log(`Skipping ${rel} (too large or not a file)`);
                continue;
              }

              const filename = `${repoName}/${rel}`;
              const hash = sha256(abs);

              // If exact same content already exists, skip
              if (await hasSameHash(filename, hash)) {
                console.log(`Unchanged, skipping: ${filename}`);
                continue;
              }

              // Otherwise delete every older copy of that filename (any content)
              await deleteAllByFilename(filename);

              // Upload (pass filename + attributes for future dedupe)
              try {
                let vf;
                if (typeof client.vectorStores?.files?.uploadAndPoll === "function") {
                  vf = await client.vectorStores.files.uploadAndPoll(
                    vsId,
                    fs.createReadStream(abs),
                    { filename, attributes: { repo: repoName, path: rel, sha256: hash } }
                  );
                } else {
                  const created = await client.files.create({
                    file: fs.createReadStream(abs),
                    purpose: "assistants",
                    filename
                  });
                  vf = await client.vectorStores.files.createAndPoll(vsId, {
                    file_id: created.id
                  });
                }

                if (vf.status !== "completed") {
                  console.error("Index failed:", filename, vf?.last_error || vf?.status);
                } else {
                  console.log("Indexed:", filename);
                }
              } catch (err) {
                const body = err?.response?.data || err?.stack || err?.message || err;
                console.error(`‚ùå Failed ${filename}:`, body);
              }
            }
          })();
          NODE
